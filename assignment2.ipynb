{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6705c7-11b8-4bc7-9348-f8dacbfc880a",
   "metadata": {},
   "source": [
    "# Data loading / preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bafbe73e-114d-4267-b282-10a46b7920e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Defining a rng with seed 256 to ensure consistency across runs \n",
    "rng = np.random.default_rng(seed=2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3cb4065b-4d32-49ba-b21f-71d13aa652b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spam_df = pd.read_csv('spambase.data', header=None)\n",
    "# without the header=None flag, the first row ended up being the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a685cab6-b73f-46a6-8d82-68a047b642f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Data Preview: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Spam Data Preview: \\n\")\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36841b48-3cc7-4db5-8dd8-2a15629bb002",
   "metadata": {},
   "source": [
    "We still need the column names, which we can get from the file spambase.names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "efd25e14-a468-4d52-b1e2-166e2f929261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list with each line in the file\n",
    "with open('spambase.names', 'r') as file:\n",
    "    file_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4fd74f15-5536-407f-ab2e-37b055f6230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column names start at line 34\n",
    "col_name_lines = file_lines[33:]\n",
    "# for col_name in col_names: print(col_name) #- > this confirms that we are starting at the correct line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4abeb4-9342-4e7c-8a3c-4a3580ed4019",
   "metadata": {},
   "source": [
    "We also need to cut off everything after the colon in each line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa077366-a5ef-4946-bba8-162a77d31c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [col_name_line.split(':')[0] for col_name_line in col_name_lines]\n",
    "# for col_name in col_names: print(col_name) # -> this confirms the success of the parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f5b0277-431f-4e86-b267-32c8bdbc426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in dataset: 58\n",
      "Number of column names acquired: 57\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns in dataset: {spam_df.shape[1]}\")\n",
    "print(f\"Number of column names acquired: {len(col_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae5e22-85ef-4f52-83a2-4ad48fe8f5f0",
   "metadata": {},
   "source": [
    "We are missing the final column, which is the label column (1 for spam, 0 for not spam). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2eee55d0-17e0-42a8-be65-f38b26bda319",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names.append(\"is_spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d839ade-85c7-4f19-934c-74f089ee4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, assign the column names\n",
    "spam_df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ff03927-dd97-4446-ba3e-ee9f05a5591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Data Preview: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  is_spam  \n",
       "0                       278        1  \n",
       "1                      1028        1  \n",
       "2                      2259        1  \n",
       "3                       191        1  \n",
       "4                       191        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Spam Data Preview: \\n\")\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89c5dc42-6d93-46ae-af97-2528f1cb0804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataset: (4601, 58)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimensions of dataset: {spam_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285ed41",
   "metadata": {},
   "source": [
    "Before going any further, we want to make sure our data is clean and won't cause issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aad47410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "word_freq_make                0\n",
      "word_freq_address             0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "word_freq_650                 0\n",
      "word_freq_lab                 0\n",
      "word_freq_labs                0\n",
      "word_freq_telnet              0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "capital_run_length_total      0\n",
      "is_spam                       0\n",
      "dtype: int64\n",
      "\n",
      "Total number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are missing values in each column of our spam_df\n",
    "print(\"Missing values per column:\")\n",
    "print(spam_df.isnull().sum())\n",
    "\n",
    "# Total count of missing values in the dataframe\n",
    "print(f\"\\nTotal number of missing values: {spam_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a24ebbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column types in spam_df:\n",
      "float64    55\n",
      "int64       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure all feature columns are numeric (ints/floats)\n",
    "print(f\"Column types in spam_df:\\n{spam_df.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3198a-2a7a-433e-ace8-c10009d31318",
   "metadata": {},
   "source": [
    "We see here that we indeed have 57 continuous features and 1 binary classifier. Now, we split the data into a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa3f09cc-88eb-4a24-8882-87517339fa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 230\n",
      "Number of testings samples: 4371\n"
     ]
    }
   ],
   "source": [
    "# Get the row indices of the dataframe \n",
    "samples = spam_df.index.tolist()   \n",
    "\n",
    "# shuffle using the rng we defined with seed 256\n",
    "rng.shuffle(samples)\n",
    "\n",
    "# divide this with a 5% / 95% split\n",
    "train_ind = samples[:round(len(samples)*0.05)]\n",
    "test_ind = samples[-round(len(samples)*0.95):]\n",
    "\n",
    "print(f\"Number of training samples: {len(train_ind)}\")\n",
    "print(f\"Number of testings samples: {len(test_ind)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9583131-de11-44e6-bd33-bb7e163dcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "\n",
    "# inputs only\n",
    "X = spam_df.iloc[:, :-1]\n",
    "\n",
    "# outputs only\n",
    "y = spam_df[\"is_spam\"]\n",
    "\n",
    "# train-test split\n",
    "X_train = X.iloc[train_ind]\n",
    "X_test = X.iloc[test_ind]\n",
    "\n",
    "y_train = y.iloc[train_ind]\n",
    "y_test = y.iloc[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3d0f3",
   "metadata": {},
   "source": [
    "Here, we want to check the label distribution in our entire dataset and the training set to ensure the training set isn't too skewed, potentially introducing bias into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c43a2b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in total dataset: is_spam\n",
      "0    0.605955\n",
      "1    0.394045\n",
      "Name: proportion, dtype: float64\n",
      "Label distribution in Training Set: is_spam\n",
      "0    0.613043\n",
      "1    0.386957\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_dist_spam_df = spam_df[\"is_spam\"].value_counts(normalize=True)\n",
    "label_dist_y_train = y_train.value_counts(normalize=True)\n",
    "print(f\"Label distribution in total dataset: {label_dist_spam_df}\")\n",
    "print(f\"Label distribution in Training Set: {label_dist_y_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab744f-dcc5-4811-969a-826b08c6addf",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "NEED TO EXPAND ON THIS SECTION (ADD MORE PRE-PROCESSING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0740b3c9-4acf-4145-a122-ef9953b977a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization\n",
    "# Calculate mean and std from training data only\n",
    "train_mean = X_train.mean()\n",
    "train_std = X_train.std()\n",
    "\n",
    "# Apply the same train_mean and train_std to both training and test sets to prevent data leakage\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_test = (X_test - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf2db2-d48e-4106-a238-b812d50198e8",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Preprocessing Methodology:\n",
    "During data preprocessing, to ensure a representative sample, we first shuffled the dataset using a fixed random seed (2026). We then performed a 5%/95% training-test data split to create a small training set of approximately 230 samples, a scenario designed to highlight the effects of overfitting. To prevent data leakage, we calculated the mean and standard deviation exclusively from the training set and used these parameters to standardize both the training and test features. Finally, we prepended a column of ones to the feature matrices to incorporate the bias term directly into our weight vector calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15769e94-ad2c-4739-9bf0-4755cdd0980c",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Here, the cost function changes as we need to include L2 Regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a46cfef-38fb-482f-a2f9-9bc0d92c97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use \"lambda_\" as lambda is a function already\n",
    "def cost_fn(x, y, w, lambda_=1e-3):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy cost (negative log-likelihood).\n",
    "    \n",
    "    Parameters:\n",
    "    x: input features, shape (N, D)\n",
    "    y: binary labels (0 or 1), shape (N,)\n",
    "    w: weights, shape (D,)\n",
    "    lamba_: regularization strength\n",
    "\n",
    "    Note: input parameters are a subset of the total dataset (mini-batch)\n",
    "    \n",
    "    Returns:\n",
    "    J: scalar cost value (lower is better)\n",
    "    \"\"\"\n",
    "    N, D = x.shape\n",
    "    \n",
    "    # Compute logits: z = x @ w, shape (N,)\n",
    "    # These are the raw predictions before applying sigmoid\n",
    "    z = np.dot(x, w)\n",
    "    \n",
    "    # Binary cross-entropy loss (mean over all samples in mini-batch)\n",
    "    # np.log1p(x) computes log(1 + x) with better numerical stability\n",
    "    # For y=1: cost is log(1 + exp(-z)) which penalizes z being too negative\n",
    "    # For y=0: cost is log(1 + exp(z)) which penalizes z being too positive\n",
    "    J = np.mean(y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z)))\n",
    "\n",
    "    # now, add regularization penalty \n",
    "    J += (lambda_ / (2 * N)) * np.sum(w[1:]**2) \n",
    "    # here, we are using the divide by two convention. We also divide by N to use the mean cost, \n",
    "    # to keep this consistent with the mean binary cross-entropy loss implemented in the code review\n",
    "    # we assume w[0] is a bias, so it is excluded here\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f512f25-7781-4d3f-9169-3cddfbe1f7a6",
   "metadata": {},
   "source": [
    "### Logistic Regression with SGD\n",
    "\n",
    "We implemented a custom LogisticRegressionSGD class that utilizes Mini-batch Stochastic Gradient Descent for optimization. To handle the intercept, a bias column of ones is prepended to the feature matrix during the fitting process. The weights are updated by computing the gradient of the cross-entropy loss function over randomized mini-batches. To combat overfitting, we integrated $L_2$ regularization (weight decay) into the gradient calculation. Additionally, we implemented numerical clipping within the sigmoid function to ensure stability against overflow, and tracked the cross-entropy loss at each epoch to visualize the model's convergence behavior.\n",
    "\n",
    "*Note: We have adopted the convention of using $\\lambda \\|\\mathbf{w}\\|^2_2$ for our L2 regularization penalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8ecde2b1-ea1d-4183-9ddf-fd00b873ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, learning_rate=10**-2, epochs=500, batch_size=16, reg_lambda=10**-3):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.w = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Clip values to avoid overflow in exp\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def compute_cross_entropy_loss(self, x, y):\n",
    "        n = x.shape[0]\n",
    "        yh = self.sigmoid(np.dot(x, self.w))\n",
    "        eps = 1e-15 \n",
    "        \n",
    "        # Cross Entropy + L2 Penalty (exclude bias w[0])\n",
    "        loss = -np.mean(y * np.log(yh + eps) + (1 - y) * np.log(1 - yh + eps))\n",
    "        l2_penalty = self.reg_lambda * np.sum(self.w[1:]**2)\n",
    "        loss += l2_penalty\n",
    "        return loss\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        n = x.shape[0]\n",
    "        yh = self.sigmoid(np.dot(x, self.w))\n",
    "        grad = np.dot(x.T, (yh - y)) / n\n",
    "        \n",
    "        # L2 Regularization Gradient: 2 * lambda * w\n",
    "        # We do not regularize the bias \n",
    "        reg_term = 2 * self.reg_lambda * self.w\n",
    "        reg_term[0] = 0 \n",
    "        grad += reg_term\n",
    "        return grad\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Add bias column here (vector of 1s)\n",
    "        n_samples = x.shape[0]\n",
    "        x_with_bias = np.column_stack([np.ones(n_samples), x])\n",
    "        \n",
    "        n_features = x_with_bias.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.loss_history = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle at the start of every epoch\n",
    "            indices = rng.permutation(n_samples)\n",
    "            x_shuffled = x_with_bias[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            # Iterate through mini-batches\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                xb = x_shuffled[i : i + self.batch_size]\n",
    "                yb = y_shuffled[i : i + self.batch_size]\n",
    "                \n",
    "                grad = self.gradient(xb, yb)\n",
    "                self.w -= self.learning_rate * grad\n",
    "            \n",
    "            # Save loss for the Task 1 training curve\n",
    "            cross_entropy_loss = self.compute_cross_entropy_loss(x_with_bias, y)\n",
    "            self.loss_history.append(cross_entropy_loss)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict_prob(self, x):\n",
    "        # Add bias column for prediction\n",
    "        x_with_bias = np.column_stack([np.ones(x.shape[0]), x])\n",
    "        return self.sigmoid(np.dot(x_with_bias, self.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91542de8-601c-4a07-85e7-e91ee0f58102",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc34576-78ca-4c12-98eb-9dac16546df7",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26b96c-d3f5-47b9-b693-bbef2bf3dbb4",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492f7aa-d756-44b2-87ee-658c2e8c40fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
